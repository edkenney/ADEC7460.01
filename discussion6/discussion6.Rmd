---
title: "Predictive Analytics & Forecasting - Discussion #6"
author: "Eric Kenney"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, results="hide", message=FALSE}
# Required Libraries
require(fpp3)
require(knitr)
require(rstudioapi)

path = dirname(documentPath())
```

## Data

I return to the data set I used in week 3 (the monthly air travel data). I reduce this data down to just air travel internal to the united states. I further reduce the data to a twenty year set and do not include the data in 2020 since that includes massive drop off in air travel due to the COVID-19 pandemic. This creates a 20 year data set I split 80/20 for training and test. I will already used the data I slimmed down and not the raw data for easy ingestion in Github.

```{r data}
air_travel = read.csv(file.path(path, "air_travel_final.csv")) %>% 
  mutate(date = yearmonth(date)) %>% 
  as_tsibble(index = date)

# Plot
air_travel %>%
  autoplot(pax/1000000) +
  labs(x = "Date", y = "Passengers (in Millions)",
       title = "Monthly Air Travel in the United States",
       subtitle = "Jan 2000 - Dec 2019")

# STL Decomposition
air_travel %>% 
  model(STL(pax)) %>% 
  components() %>% 
  autoplot() +
  labs(x = "Date", y = "",
       title = "STL Decomp for Monthly Air Travel")
```

## Modeling

In the previous discussion I created an ETS and ARIMA model. The automatic selection from the fable package created models that were able to forecast to within 2 percent of the test set. I will now build a Neural Network Time Series Forecast.

```{r model, warning=FALSE}
#Build training and test sets
total_obs = dim(air_travel)[1]
test_obs = round(total_obs*0.2)
train_obs = total_obs - test_obs

air_travel_train = head(air_travel, train_obs)
air_travel_test = tail(air_travel, test_obs)

air_travel_nn = air_travel_train %>% 
  model(NNETAR(pax))

report(air_travel_nn)

air_travel_nn %>% 
  gg_tsresiduals(lag_max = 12) +
  labs(title = "NN Model Residuals")

# Run a Ljung_box test, value is not significant
augment(air_travel_nn) %>% 
  features(.innov, ljung_box, lag = 12)
```
There are the same issues in the previous view of the data with the outlier after 9/11. There are some lag spikes, but a Ljung-Box test shows a p-value of essentially zero.

## Prediction

I will create the final model and conduct predictions.

```{r predict, warning=FALSE}
air_travel_model = air_travel_train %>% 
  model(
    `ETS` = ETS(pax),
    `ARIMA` = ARIMA(pax),
    `Neural Network` = NNETAR(pax)
  )

glance(air_travel_model)

air_travel_forecast = air_travel_model %>% 
  forecast(air_travel_test)
  
# Plot
air_travel_forecast %>% 
  autoplot(level = NULL, lwd = 1) +
  labs(x = "Date", y = "Passengers (in Millions)",
       title = "Model Forecasts",
       subtitle = "Test Set is the Black Line") +
  guides(color = guide_legend(title = "Model")) +
  geom_line(data = air_travel_test,
            aes(x = date, y = pax),
            lwd = 1.25)

# Calculate Accuracy
accuracy_table = air_travel_forecast %>% 
  accuracy(air_travel_test) %>% 
  select(-c(.type, MASE, RMSSE)) %>% 
  rename("Model" = ".model")

kable(accuracy_table,
      format = "markdown",
      caption = "Model Accuracy")
```

With a longer time horizon than my previous discussion (6 months vs. 4 years), the forecasts do not do as well as time goes on. However, all three models perform well on the data with a MAPE of below 10%. The ETS model still performs the best, but the neural net performs better than the ARIMA model.
